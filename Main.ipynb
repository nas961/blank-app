{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nas961/blank-app/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_curve, auc, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from xgboost import XGBClassifier\n",
        "from google.colab import drive\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from itertools import cycle\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import joblib\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# Vérifier si CUDA est disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Téléchargement des stop words pour toutes les langues disponibles\n",
        "nltk.download('stopwords', quiet=True)\n",
        "all_stopwords = set()\n",
        "for lang in stopwords.fileids():\n",
        "    all_stopwords.update(stopwords.words(lang))\n",
        "\n",
        "# Monter Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Chemins vers les fichiers dans Google Drive\n",
        "csv_dir = '/content/drive/MyDrive/DATA'\n",
        "save_dir = '/content/drive/MyDrive/Text_Classification'\n",
        "\n",
        "# Créer le dossier de sauvegarde s'il n'existe pas\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    try:\n",
        "        X_train = pd.read_csv(os.path.join(csv_dir, 'X_train.csv'))\n",
        "        Y_train = pd.read_csv(os.path.join(csv_dir, 'Y_train.csv'))\n",
        "        X_test = pd.read_csv(os.path.join(csv_dir, 'X_test.csv'))\n",
        "\n",
        "        print(\"Colonnes dans X_train:\", X_train.columns)\n",
        "        print(\"Colonnes dans Y_train:\", Y_train.columns)\n",
        "\n",
        "        if 'productid' in X_train.columns and 'productid' in Y_train.columns:\n",
        "            data = pd.merge(X_train, Y_train, on='productid')\n",
        "        else:\n",
        "            print(\"Attention: 'productid' n'est pas présent dans les dataframes. On suppose qu'ils sont déjà alignés.\")\n",
        "            X_train = X_train.reset_index(drop=True)\n",
        "            Y_train = Y_train.reset_index(drop=True)\n",
        "            data = pd.concat([X_train, Y_train], axis=1)\n",
        "\n",
        "        required_columns = ['designation', 'description', 'prdtypecode']\n",
        "        for col in required_columns:\n",
        "            if col not in data.columns:\n",
        "                raise ValueError(f\"La colonne '{col}' n'est pas présente dans les données fusionnées.\")\n",
        "\n",
        "        # Gestion des valeurs NaN\n",
        "        data['designation'] = data['designation'].fillna('')\n",
        "        data['description'] = data['description'].fillna('')\n",
        "\n",
        "        # Combiner 'designation' et 'description' pour une meilleure représentation du texte\n",
        "        X = data['designation'] + \" \" + data['description']\n",
        "        y = data['prdtypecode']\n",
        "\n",
        "        le = LabelEncoder()\n",
        "        y_encoded = le.fit_transform(y)\n",
        "\n",
        "        print(\"Vectorisation TF-IDF...\")\n",
        "        tfidf = TfidfVectorizer(max_features=10000, stop_words=list(all_stopwords), ngram_range=(1, 2))\n",
        "        X_tfidf = tfidf.fit_transform(X)\n",
        "\n",
        "        print(\"Réduction de dimensionnalité avec TruncatedSVD...\")\n",
        "        svd = TruncatedSVD(n_components=300, random_state=42)\n",
        "        X_svd = svd.fit_transform(X_tfidf)\n",
        "\n",
        "        print(\"Standardisation des données...\")\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_svd)\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "        # Calcul des poids des classes\n",
        "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "        class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "        # Convertir les données en tensors PyTorch et les déplacer sur le GPU\n",
        "        X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "        y_train_tensor = torch.LongTensor(y_train).to(device)\n",
        "        X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
        "        y_val_tensor = torch.LongTensor(y_val).to(device)\n",
        "\n",
        "        return X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor, le.classes_, class_weight_dict, tfidf, svd, scaler\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors du chargement et de la préparation des données : {str(e)}\")\n",
        "        raise\n",
        "\n",
        "class SVMWithGPU(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(SVMWithGPU, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "def create_models(input_dim, num_classes, class_weight_dict):\n",
        "    svm_model = SVMWithGPU(input_dim, num_classes).to(device)\n",
        "    rf_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight=class_weight_dict, n_jobs=-1)\n",
        "    lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight=class_weight_dict, n_jobs=-1)\n",
        "    xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, tree_method='hist', device='cuda')\n",
        "\n",
        "    return [\n",
        "        (svm_model, \"SVM\"),\n",
        "        (rf_model, \"Random Forest\"),\n",
        "        (lr_model, \"Logistic Regression\"),\n",
        "        (xgb_model, \"XGBoost\")\n",
        "    ]\n",
        "\n",
        "def train_and_evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
        "    print(f\"Entraînement du modèle {model_name}...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        if model_name == \"SVM\":\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = torch.optim.Adam(model.parameters())\n",
        "            num_epochs = 10\n",
        "            batch_size = 64\n",
        "\n",
        "            train_dataset = TensorDataset(X_train, y_train)\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                for batch_X, batch_y in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                y_train_pred = model(X_train).argmax(dim=1)\n",
        "                y_val_pred = model(X_val).argmax(dim=1)\n",
        "        elif model_name == \"XGBoost\":\n",
        "            X_train_cpu = X_train.cpu().numpy()\n",
        "            y_train_cpu = y_train.cpu().numpy()\n",
        "            X_val_cpu = X_val.cpu().numpy()\n",
        "            y_val_cpu = y_val.cpu().numpy()\n",
        "\n",
        "            model.fit(X_train_cpu, y_train_cpu, eval_set=[(X_val_cpu, y_val_cpu)], verbose=False)\n",
        "            y_train_pred = model.predict(X_train_cpu)\n",
        "            y_val_pred = model.predict(X_val_cpu)\n",
        "        else:\n",
        "            X_train_cpu = X_train.cpu().numpy()\n",
        "            y_train_cpu = y_train.cpu().numpy()\n",
        "            X_val_cpu = X_val.cpu().numpy()\n",
        "\n",
        "            model.fit(X_train_cpu, y_train_cpu)\n",
        "            y_train_pred = model.predict(X_train_cpu)\n",
        "            y_val_pred = model.predict(X_val_cpu)\n",
        "\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "\n",
        "        y_train_cpu = y_train.cpu().numpy()\n",
        "        y_val_cpu = y_val.cpu().numpy()\n",
        "\n",
        "        if isinstance(y_train_pred, torch.Tensor):\n",
        "            y_train_pred = y_train_pred.cpu().numpy()\n",
        "        if isinstance(y_val_pred, torch.Tensor):\n",
        "            y_val_pred = y_val_pred.cpu().numpy()\n",
        "\n",
        "        train_accuracy = accuracy_score(y_train_cpu, y_train_pred)\n",
        "        train_f1 = f1_score(y_train_cpu, y_train_pred, average='weighted')\n",
        "\n",
        "        val_accuracy = accuracy_score(y_val_cpu, y_val_pred)\n",
        "        val_f1 = f1_score(y_val_cpu, y_val_pred, average='weighted')\n",
        "\n",
        "        print(f\"\\nRésultats pour {model_name}:\")\n",
        "        print(f\"Temps d'entraînement: {training_time:.2f} secondes\")\n",
        "        print(f\"Accuracy d'entraînement: {train_accuracy:.4f}\")\n",
        "        print(f\"F1 Score d'entraînement: {train_f1:.4f}\")\n",
        "        print(f\"Accuracy de validation: {val_accuracy:.4f}\")\n",
        "        print(f\"F1 Score de validation: {val_f1:.4f}\")\n",
        "        print(\"\\nRapport de classification (validation):\")\n",
        "        print(classification_report(y_val_cpu, y_val_pred))\n",
        "\n",
        "        return model, val_accuracy, val_f1\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de l'entraînement du modèle {model_name}: {str(e)}\")\n",
        "        return None, 0, 0\n",
        "\n",
        "def plot_roc_curve(model, X_val, y_val, model_name, class_names):\n",
        "    try:\n",
        "        n_classes = len(np.unique(y_val.cpu().numpy()))\n",
        "\n",
        "        if n_classes == 2:\n",
        "            if hasattr(model, \"predict_proba\"):\n",
        "                y_score = model.predict_proba(X_val.cpu().numpy())[:, 1]\n",
        "            else:\n",
        "                y_score = model(X_val).detach().cpu().numpy()[:, 1]\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_val.cpu().numpy(), y_score)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "            plt.xlim([0.0, 1.0])\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f'ROC Curve - {model_name}')\n",
        "            plt.legend(loc=\"lower right\")\n",
        "        else:\n",
        "            fpr = dict()\n",
        "            tpr = dict()\n",
        "            roc_auc = dict()\n",
        "\n",
        "            if hasattr(model, \"predict_proba\"):\n",
        "                y_score = model.predict_proba(X_val.cpu().numpy())\n",
        "            else:\n",
        "                y_score = model(X_val).detach().cpu().numpy()\n",
        "\n",
        "            y_test_bin = label_binarize(y_val.cpu().numpy(), classes=np.arange(n_classes))\n",
        "\n",
        "            for i in range(n_classes):\n",
        "                fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'yellow', 'purple'])\n",
        "            for i, color in zip(range(n_classes), colors):\n",
        "                plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                         label=f'ROC curve of class {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "            plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "            plt.xlim([0.0, 1.0])\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f'Multi-class ROC Curve - {model_name}')\n",
        "            plt.legend(loc=\"lower right\")\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, f'{model_name}_ROC_curve.png'))\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la création de la courbe ROC pour {model_name}: {str(e)}\")\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.savefig(os.path.join(save_dir, f'{model_name}_confusion_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "def plot_feature_importance(model, feature_names, model_name):\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.title(f\"Feature Importances - {model_name}\")\n",
        "        plt.bar(range(10), importances[indices][:10])\n",
        "        plt.xticks(range(10), [feature_names[i] for i in indices[:10]], rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, f'{model_name}_feature_importance.png'))\n",
        "        plt.close()\n",
        "\n",
        "def train_and_evaluate_all_models(models, X_train, X_val, y_train, y_val, class_names, feature_names):\n",
        "    results = []\n",
        "    total_models = len(models)\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, (model, name) in enumerate(models, 1):\n",
        "        print(f\"\\nEntraînement du modèle {i}/{total_models}: {name}\")\n",
        "        trained_model, val_accuracy, val_f1 = train_and_evaluate_model(model, X_train, y_train, X_val, y_val, name)\n",
        "        results.append((name, val_accuracy, val_f1))\n",
        "\n",
        "        if trained_model is not None:\n",
        "            plot_roc_curve(trained_model, X_val, y_val, name, class_names)\n",
        "\n",
        "            if name != \"SVM\":  # SVM personnalisé n'a pas de méthode predict\n",
        "                y_val_pred = trained_model.predict(X_val.cpu().numpy())\n",
        "            else:\n",
        "                y_val_pred = trained_model(X_val).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "            plot_confusion_matrix(y_val.cpu().numpy(), y_val_pred, class_names, name)\n",
        "\n",
        "            if name in [\"Random Forest\", \"XGBoost\"]:\n",
        "                plot_feature_importance(trained_model, feature_names, name)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        avg_time_per_model = elapsed_time / i\n",
        "        eta = avg_time_per_model * (total_models - i)\n",
        "\n",
        "        print(f\"Progression : {i}/{total_models} modèles\")\n",
        "        print(f\"Temps écoulé : {elapsed_time:.2f} secondes\")\n",
        "        print(f\"Temps estimé restant : {eta:.2f} secondes\")\n",
        "\n",
        "    print(\"\\nEntraînement de tous les modèles terminé!\")\n",
        "    return results\n",
        "\n",
        "def plot_model_comparison(results):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    names = [r[0] for r in results]\n",
        "    accuracies = [r[1] for r in results]\n",
        "    f1_scores = [r[2] for r in results]\n",
        "\n",
        "    x = range(len(names))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    rects1 = ax.bar([i - width/2 for i in x], accuracies, width, label='Accuracy', color='skyblue')\n",
        "    rects2 = ax.bar([i + width/2 for i in x], f1_scores, width, label='F1 Score', color='lightgreen')\n",
        "\n",
        "    ax.set_ylabel('Scores')\n",
        "    ax.set_title('Comparaison des performances des modèles')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "\n",
        "    ax.set_ylim(0.70, 1.00)\n",
        "\n",
        "    def autolabel(rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate(f'{height:.3f}',\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 3),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom', rotation=90)\n",
        "\n",
        "    autolabel(rects1)\n",
        "    autolabel(rects2)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'model_comparison.png'))\n",
        "    plt.close()\n",
        "\n",
        "def plot_training_time(training_times):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    names = list(training_times.keys())\n",
        "    times = list(training_times.values())\n",
        "\n",
        "    plt.bar(names, times, color='lightcoral')\n",
        "    plt.title('Temps d\\'entraînement par modèle')\n",
        "    plt.xlabel('Modèles')\n",
        "    plt.ylabel('Temps (secondes)')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    for i, v in enumerate(times):\n",
        "        plt.text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'training_time_comparison.png'))\n",
        "    plt.close()\n",
        "\n",
        "def create_description_file(results, training_times):\n",
        "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    with open(os.path.join(save_dir, f'{current_date}-ML_pipeline_description.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write(\"Description de la pipeline de Machine Learning\\n\")\n",
        "        f.write(\"===========================================\\n\\n\")\n",
        "        f.write(\"1. Chargement et préparation des données\\n\")\n",
        "        f.write(\"2. Gestion des valeurs NaN dans les colonnes textuelles\\n\")\n",
        "        f.write(\"3. Combinaison de 'designation' et 'description' pour une meilleure représentation du texte\\n\")\n",
        "        f.write(\"4. Prétraitement du texte avec TfidfVectorizer, utilisation de bi-grammes et suppression des stop words de toutes les langues\\n\")\n",
        "        f.write(\"5. Réduction de dimensionnalité avec TruncatedSVD\\n\")\n",
        "        f.write(\"6. Standardisation des données avec StandardScaler\\n\")\n",
        "        f.write(\"7. Encodage des labels avec LabelEncoder\\n\")\n",
        "        f.write(\"8. Calcul et application des poids des classes pour gérer le déséquilibre des classes\\n\")\n",
        "        f.write(\"9. Utilisation du GPU pour l'entraînement des modèles\\n\")\n",
        "        f.write(\"10. Entraînement de quatre modèles : SVM (personnalisé avec PyTorch), Random Forest, Logistic Regression, XGBoost\\n\")\n",
        "        f.write(\"11. Évaluation des modèles avec des métriques de performance (Accuracy, F1 Score)\\n\")\n",
        "        f.write(\"12. Génération de courbes ROC pour chaque modèle\\n\")\n",
        "        f.write(\"13. Création de matrices de confusion pour chaque modèle\\n\")\n",
        "        f.write(\"14. Visualisation de l'importance des caractéristiques pour Random Forest et XGBoost\\n\")\n",
        "        f.write(\"15. Comparaison visuelle des performances des modèles\\n\")\n",
        "        f.write(\"16. Analyse des temps d'entraînement\\n\")\n",
        "        f.write(\"\\nRésultats :\\n\")\n",
        "        for name, accuracy, f1 in results:\n",
        "            f.write(f\"{name}: Accuracy = {accuracy:.4f}, F1 Score = {f1:.4f}, Temps d'entraînement = {training_times[name]:.2f} secondes\\n\")\n",
        "        f.write(\"\\nNote: SVM est implémenté avec PyTorch pour utiliser le GPU. XGBoost est configuré pour utiliser le GPU.\\n\")\n",
        "        f.write(\"Tous les résultats et visualisations sont sauvegardés dans le dossier 'Text_Classification' sur Google Drive.\\n\")\n",
        "\n",
        "    print(f\"Les résultats et visualisations ont été sauvegardés dans le dossier 'Text_Classification' sur Google Drive.\")\n",
        "    print(f\"Un fichier de description '{current_date}-ML_pipeline_description.txt' a été créé dans le même dossier.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        X_train, X_val, y_train, y_val, class_names, class_weight_dict, tfidf, svd, scaler = load_and_prepare_data()\n",
        "        input_dim = X_train.shape[1]\n",
        "        num_classes = len(class_names)\n",
        "        models = create_models(input_dim, num_classes, class_weight_dict)\n",
        "\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "        training_times = {}\n",
        "        results = []\n",
        "\n",
        "        for model, name in models:\n",
        "            start_time = time.time()\n",
        "            trained_model, val_accuracy, val_f1 = train_and_evaluate_model(model, X_train, y_train, X_val, y_val, name)\n",
        "            end_time = time.time()\n",
        "            training_time = end_time - start_time\n",
        "            training_times[name] = training_time\n",
        "            results.append((name, val_accuracy, val_f1))\n",
        "\n",
        "            if trained_model is not None:\n",
        "                plot_roc_curve(trained_model, X_val, y_val, name, class_names)\n",
        "\n",
        "                if name != \"SVM\":\n",
        "                    y_val_pred = trained_model.predict(X_val.cpu().numpy())\n",
        "                else:\n",
        "                    y_val_pred = trained_model(X_val).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "                plot_confusion_matrix(y_val.cpu().numpy(), y_val_pred, class_names, name)\n",
        "\n",
        "                if name in [\"Random Forest\", \"XGBoost\"]:\n",
        "                    plot_feature_importance(trained_model, feature_names, name)\n",
        "\n",
        "        plot_model_comparison(results)\n",
        "        plot_training_time(training_times)\n",
        "        create_description_file(results, training_times)\n",
        "\n",
        "        # Sauvegarder les meilleurs modèles\n",
        "        best_model = max(results, key=lambda x: x[2])[0]  # Choisir le modèle avec le meilleur F1 score\n",
        "        for model, name in models:\n",
        "            if name == best_model:\n",
        "                print(f\"Sauvegarde du meilleur modèle : {name}\")\n",
        "                model_filename = os.path.join(save_dir, f'{name}_best_model.joblib')\n",
        "                joblib.dump(model, model_filename)\n",
        "                print(f\"Modèle sauvegardé : {model_filename}\")\n",
        "\n",
        "        # Sauvegarder le TfidfVectorizer, TruncatedSVD et le StandardScaler\n",
        "        tfidf_filename = os.path.join(save_dir, 'tfidf_vectorizer.joblib')\n",
        "        svd_filename = os.path.join(save_dir, 'truncated_svd.joblib')\n",
        "        scaler_filename = os.path.join(save_dir, 'standard_scaler.joblib')\n",
        "        joblib.dump(tfidf, tfidf_filename)\n",
        "        joblib.dump(svd, svd_filename)\n",
        "        joblib.dump(scaler, scaler_filename)\n",
        "        print(f\"TfidfVectorizer sauvegardé : {tfidf_filename}\")\n",
        "        print(f\"TruncatedSVD sauvegardé : {svd_filename}\")\n",
        "        print(f\"StandardScaler sauvegardé : {scaler_filename}\")\n",
        "\n",
        "        # Prédiction sur l'ensemble de test\n",
        "        X_test = pd.read_csv(os.path.join(csv_dir, 'X_test.csv'))\n",
        "        X_test['designation'] = X_test['designation'].fillna('')\n",
        "        X_test['description'] = X_test['description'].fillna('')\n",
        "        X_test_text = X_test['designation'] + \" \" + X_test['description']\n",
        "        X_test_tfidf = tfidf.transform(X_test_text)\n",
        "        X_test_svd = svd.transform(X_test_tfidf)\n",
        "        X_test_scaled = scaler.transform(X_test_svd)\n",
        "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "\n",
        "        best_model_name = best_model\n",
        "        best_model = joblib.load(os.path.join(save_dir, f'{best_model_name}_best_model.joblib'))\n",
        "\n",
        "        if best_model_name == \"SVM\":\n",
        "            y_test_pred = best_model(X_test_tensor).argmax(dim=1).cpu().numpy()\n",
        "        else:\n",
        "            y_test_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "        # Convertir les prédictions en catégories originales\n",
        "        le = LabelEncoder()\n",
        "        le.classes_ = class_names\n",
        "        y_test_pred_categories = le.inverse_transform(y_test_pred)\n",
        "\n",
        "        # Créer un DataFrame avec les prédictions\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'productid': X_test['productid'],\n",
        "            'predicted_category': y_test_pred_categories\n",
        "        })\n",
        "\n",
        "        # Sauvegarder les prédictions dans un fichier CSV\n",
        "        predictions_filename = os.path.join(save_dir, 'test_predictions.csv')\n",
        "        predictions_df.to_csv(predictions_filename, index=False)\n",
        "        print(f\"Prédictions sur l'ensemble de test sauvegardées : {predictions_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Une erreur s'est produite lors de l'exécution du script : {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h83gUS4AYSNl",
        "outputId": "795bc2d8-86a7-4361-b3dc-8a1bb90abdf8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Colonnes dans X_train: Index(['Unnamed: 0', 'designation', 'description', 'productid', 'imageid'], dtype='object')\n",
            "Colonnes dans Y_train: Index(['Unnamed: 0', 'prdtypecode'], dtype='object')\n",
            "Attention: 'productid' n'est pas présent dans les dataframes. On suppose qu'ils sont déjà alignés.\n",
            "Vectorisation TF-IDF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aldiz', 'baiknya', 'baizik', 'berkali', 'bukatzeko', 'edota', 'eze', 'ezpabere', 'ezpada', 'ezperen', 'gainera', 'gainerontzean', 'guztiz', 'hainbestez', 'horra', 'kali', 'kurangnya', 'mata', 'olah', 'onların', 'ordea', 'osterantzean', 'printr', 'sekurang', 'setidak', 'tama', 'tidaknya', 'δι', 'арбаң', 'арсалаң', 'афташ', 'бай', 'бале', 'баски', 'батыр', 'баҳри', 'болои', 'бүгжең', 'бұтыр', 'валекин', 'вақте', 'вой', 'вуҷуди', 'гар', 'гарчанде', 'далаң', 'даме', 'ербелең', 'жалт', 'жұлт', 'карда', 'кошки', 'куя', 'күңгір', 'кӣ', 'магар', 'майлаш', 'митың', 'модоме', 'нияти', 'онан', 'оре', 'паһ', 'рӯи', 'салаң', 'сар', 'сұлаң', 'сұрт', 'тарбаң', 'тразе', 'ту', 'тыржың', 'тұрс', 'хом', 'хуб', 'чаро', 'чи', 'чун', 'чунон', 'шарте', 'шұңқ', 'ыржың', 'қадар', 'қайқаң', 'қалт', 'қаңғыр', 'қаңқ', 'қош', 'қызараң', 'құйқаң', 'құлт', 'құңқ', 'ұрс', 'ҳай', 'ҳамин', 'ҳатто', 'ҳо', 'ҳол', 'ҳолате', 'әттеген', 'ӯим', 'באיזו', 'בו', 'במקום', 'בשעה', 'הסיבה', 'לאיזו', 'למקום', 'מאיזו', 'מידה', 'מקום', 'סיבה', 'שבגללה', 'שבו', 'תכלית', 'آمين', 'أب', 'أخ', 'أفعل', 'أفعله', 'ؤلاء', 'إل', 'إم', 'ات', 'اتان', 'ارتد', 'ان', 'انفك', 'برح', 'تان', 'تبد', 'تحو', 'تعل', 'حد', 'حم', 'حي', 'خب', 'ذار', 'سيما', 'صه', 'ظل', 'ظن', 'عد', 'قط', 'مر', 'مكان', 'مكانكن', 'نب', 'هات', 'هب', 'واها', 'وراء', 'अक', 'अग', 'अझ', 'अन', 'अर', 'आजक', 'आत', 'आद', 'आफ', 'आय', 'ईक', 'उद', 'उनक', 'उनल', 'उह', 'एउट', 'एन', 'कत', 'कम', 'कस', 'कसर', 'कह', 'गत', 'गय', 'गर', 'चम', 'छन', 'जत', 'जबक', 'जस', 'जसक', 'जसब', 'जसम', 'जसल', 'जह', 'तत', 'तथ', 'तदन', 'तप', 'तवम', 'नज', 'नत', 'नभन', 'नय', 'पक', 'पछ', 'पन', 'पय', 'पर', 'पष', 'पह', 'बन', 'बर', 'भएक', 'भय', 'भव', 'मल', 'यत', 'यथ', 'यद', 'यप', 'यसक', 'यसपछ', 'यसब', 'यसर', 'यह', 'रण', 'रत', 'रमश', 'रह', 'लस', 'वर', 'सक', 'सट', 'सध', 'सपछ', 'सब', 'सम', 'सर', 'सह', 'हन', 'हर', 'हरण', 'অথব', 'অন', 'অবধ', 'অবশ', 'অর', 'আগ', 'আছ', 'আদ', 'আপন', 'আব', 'আম', 'আমর', 'ইত', 'ইহ', 'উই', 'উক', 'উচ', 'উত', 'উন', 'এক', 'একট', 'একব', 'এখ', 'এট', 'এতট', 'এদ', 'এমনক', 'ওক', 'ওখ', 'ওদ', 'ওয', 'ওয়', 'কট', 'কব', 'কমন', 'কয', 'কর', 'করছ', 'করত', 'করব', 'করল', 'কয়', 'খত', 'চল', 'জনক', 'তথ', 'তব', 'তর', 'থম', 'ধর', 'নঐ', 'নও', 'নজন', 'নত', 'নয', 'পক', 'পষ', 'ফল', 'বক', 'বদল', 'বয', 'বর', 'বল', 'বলত', 'বস', 'বহ', 'মত', 'মধ', 'মন', 'যওজ', 'যতট', 'যথ', 'যদ', 'যন', 'যবহ', 'যভ', 'যম', 'রও', 'রণ', 'রত', 'রথম', 'রপর', 'রভ', 'রয', 'রযন', 'লক', 'ষয', 'সঙ', 'সম', 'সমস', 'হইত', 'হইব', 'হইয', 'হওয', 'হচ', 'হব', 'হয', 'ἀλλ'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Réduction de dimensionnalité avec TruncatedSVD...\n",
            "Standardisation des données...\n",
            "Entraînement du modèle SVM...\n",
            "\n",
            "Résultats pour SVM:\n",
            "Temps d'entraînement: 17.35 secondes\n",
            "Accuracy d'entraînement: 0.7516\n",
            "F1 Score d'entraînement: 0.7543\n",
            "Accuracy de validation: 0.7350\n",
            "F1 Score de validation: 0.7377\n",
            "\n",
            "Rapport de classification (validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.61      0.41       623\n",
            "           1       0.68      0.54      0.60       502\n",
            "           2       0.73      0.67      0.70       336\n",
            "           3       0.86      0.78      0.82       166\n",
            "           4       0.70      0.67      0.69       534\n",
            "           5       0.86      0.83      0.84       791\n",
            "           6       0.54      0.38      0.44       153\n",
            "           7       0.60      0.52      0.56       974\n",
            "           8       0.53      0.41      0.47       414\n",
            "           9       0.80      0.86      0.83      1009\n",
            "          10       0.90      0.88      0.89       161\n",
            "          11       0.66      0.61      0.63       498\n",
            "          12       0.70      0.65      0.67       648\n",
            "          13       0.75      0.73      0.74      1015\n",
            "          14       0.87      0.86      0.87       861\n",
            "          15       0.80      0.75      0.77       161\n",
            "          16       0.70      0.75      0.72       999\n",
            "          17       0.79      0.68      0.73       165\n",
            "          18       0.71      0.75      0.73       952\n",
            "          19       0.77      0.68      0.72       955\n",
            "          20       0.71      0.69      0.70       284\n",
            "          21       0.84      0.85      0.85       998\n",
            "          22       0.67      0.64      0.65       518\n",
            "          23       0.93      0.95      0.94      2042\n",
            "          24       0.67      0.63      0.65       499\n",
            "          25       0.72      0.67      0.69       552\n",
            "          26       0.97      0.98      0.98       174\n",
            "\n",
            "    accuracy                           0.73     16984\n",
            "   macro avg       0.73      0.70      0.72     16984\n",
            "weighted avg       0.75      0.73      0.74     16984\n",
            "\n",
            "Entraînement du modèle Random Forest...\n",
            "\n",
            "Résultats pour Random Forest:\n",
            "Temps d'entraînement: 464.04 secondes\n",
            "Accuracy d'entraînement: 0.9739\n",
            "F1 Score d'entraînement: 0.9748\n",
            "Accuracy de validation: 0.7505\n",
            "F1 Score de validation: 0.7496\n",
            "\n",
            "Rapport de classification (validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.52      0.43       623\n",
            "           1       0.74      0.57      0.64       502\n",
            "           2       0.76      0.73      0.74       336\n",
            "           3       0.98      0.73      0.84       166\n",
            "           4       0.72      0.71      0.71       534\n",
            "           5       0.88      0.87      0.87       791\n",
            "           6       0.65      0.46      0.54       153\n",
            "           7       0.63      0.54      0.58       974\n",
            "           8       0.55      0.44      0.49       414\n",
            "           9       0.81      0.87      0.84      1009\n",
            "          10       0.93      0.75      0.83       161\n",
            "          11       0.81      0.57      0.67       498\n",
            "          12       0.74      0.65      0.69       648\n",
            "          13       0.69      0.82      0.75      1015\n",
            "          14       0.89      0.88      0.89       861\n",
            "          15       0.85      0.75      0.79       161\n",
            "          16       0.73      0.76      0.75       999\n",
            "          17       0.87      0.62      0.73       165\n",
            "          18       0.65      0.78      0.71       952\n",
            "          19       0.72      0.68      0.70       955\n",
            "          20       0.71      0.76      0.73       284\n",
            "          21       0.80      0.87      0.83       998\n",
            "          22       0.80      0.65      0.71       518\n",
            "          23       0.90      0.97      0.94      2042\n",
            "          24       0.74      0.63      0.68       499\n",
            "          25       0.71      0.67      0.69       552\n",
            "          26       0.98      0.97      0.98       174\n",
            "\n",
            "    accuracy                           0.75     16984\n",
            "   macro avg       0.76      0.71      0.73     16984\n",
            "weighted avg       0.76      0.75      0.75     16984\n",
            "\n",
            "Entraînement du modèle Logistic Regression...\n",
            "\n",
            "Résultats pour Logistic Regression:\n",
            "Temps d'entraînement: 23.47 secondes\n",
            "Accuracy d'entraînement: 0.7504\n",
            "F1 Score d'entraînement: 0.7552\n",
            "Accuracy de validation: 0.7272\n",
            "F1 Score de validation: 0.7330\n",
            "\n",
            "Rapport de classification (validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.61      0.42       623\n",
            "           1       0.62      0.56      0.59       502\n",
            "           2       0.69      0.73      0.71       336\n",
            "           3       0.82      0.81      0.81       166\n",
            "           4       0.69      0.71      0.70       534\n",
            "           5       0.89      0.82      0.85       791\n",
            "           6       0.30      0.56      0.39       153\n",
            "           7       0.65      0.41      0.50       974\n",
            "           8       0.45      0.49      0.47       414\n",
            "           9       0.83      0.85      0.84      1009\n",
            "          10       0.73      0.90      0.81       161\n",
            "          11       0.62      0.67      0.64       498\n",
            "          12       0.67      0.66      0.66       648\n",
            "          13       0.78      0.70      0.74      1015\n",
            "          14       0.87      0.87      0.87       861\n",
            "          15       0.58      0.86      0.69       161\n",
            "          16       0.76      0.70      0.73       999\n",
            "          17       0.50      0.75      0.60       165\n",
            "          18       0.77      0.71      0.74       952\n",
            "          19       0.82      0.63      0.72       955\n",
            "          20       0.67      0.74      0.70       284\n",
            "          21       0.90      0.84      0.87       998\n",
            "          22       0.63      0.68      0.65       518\n",
            "          23       0.97      0.90      0.94      2042\n",
            "          24       0.59      0.68      0.63       499\n",
            "          25       0.67      0.67      0.67       552\n",
            "          26       0.98      0.97      0.97       174\n",
            "\n",
            "    accuracy                           0.73     16984\n",
            "   macro avg       0.69      0.72      0.70     16984\n",
            "weighted avg       0.75      0.73      0.73     16984\n",
            "\n",
            "Entraînement du modèle XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [19:51:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Résultats pour XGBoost:\n",
            "Temps d'entraînement: 40.60 secondes\n",
            "Accuracy d'entraînement: 0.9741\n",
            "F1 Score d'entraînement: 0.9750\n",
            "Accuracy de validation: 0.7561\n",
            "F1 Score de validation: 0.7567\n",
            "\n",
            "Rapport de classification (validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.50      0.42       623\n",
            "           1       0.67      0.57      0.62       502\n",
            "           2       0.80      0.73      0.76       336\n",
            "           3       0.96      0.77      0.86       166\n",
            "           4       0.72      0.70      0.71       534\n",
            "           5       0.89      0.86      0.88       791\n",
            "           6       0.70      0.38      0.49       153\n",
            "           7       0.55      0.57      0.56       974\n",
            "           8       0.53      0.44      0.48       414\n",
            "           9       0.80      0.87      0.83      1009\n",
            "          10       0.94      0.81      0.87       161\n",
            "          11       0.74      0.64      0.68       498\n",
            "          12       0.71      0.69      0.70       648\n",
            "          13       0.75      0.80      0.77      1015\n",
            "          14       0.88      0.88      0.88       861\n",
            "          15       0.91      0.73      0.81       161\n",
            "          16       0.72      0.78      0.75       999\n",
            "          17       0.86      0.64      0.73       165\n",
            "          18       0.70      0.77      0.73       952\n",
            "          19       0.72      0.69      0.71       955\n",
            "          20       0.77      0.77      0.77       284\n",
            "          21       0.86      0.87      0.86       998\n",
            "          22       0.72      0.66      0.69       518\n",
            "          23       0.95      0.97      0.96      2042\n",
            "          24       0.75      0.70      0.72       499\n",
            "          25       0.71      0.66      0.68       552\n",
            "          26       0.98      0.97      0.97       174\n",
            "\n",
            "    accuracy                           0.76     16984\n",
            "   macro avg       0.76      0.72      0.74     16984\n",
            "weighted avg       0.76      0.76      0.76     16984\n",
            "\n",
            "Les résultats et visualisations ont été sauvegardés dans le dossier 'Text_Classification' sur Google Drive.\n",
            "Un fichier de description '2024-10-14-ML_pipeline_description.txt' a été créé dans le même dossier.\n",
            "Sauvegarde du meilleur modèle : XGBoost\n",
            "Modèle sauvegardé : /content/drive/MyDrive/Text_Classification/XGBoost_best_model.joblib\n",
            "TfidfVectorizer sauvegardé : /content/drive/MyDrive/Text_Classification/tfidf_vectorizer.joblib\n",
            "TruncatedSVD sauvegardé : /content/drive/MyDrive/Text_Classification/truncated_svd.joblib\n",
            "StandardScaler sauvegardé : /content/drive/MyDrive/Text_Classification/standard_scaler.joblib\n",
            "Prédictions sur l'ensemble de test sauvegardées : /content/drive/MyDrive/Text_Classification/test_predictions.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNdR+ZB2m79ZScY+wRFEz2p",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}